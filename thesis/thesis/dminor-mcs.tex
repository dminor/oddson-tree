\documentclass[mcs]{scsthesis}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}

\title {Biased Nearest Neighbour Search}
\author {Daniel Minor}
\thesissupervisor {Patrick Morin}
\director {Douglas Howe}

\begin{document}

\newtheorem*{thm}{Theorem}[section]

\beforepreface

\prefacesection

\chapter*{Abstract}

The nearest neighbour search problem is given a set of points and a search
point, return the closest point in the set to the search point. In the biased
version of this problem a probability distribution over the search points is
available that can be used to answer searches more efficiently. This thesis
contains experimental results showing that practical implementations of data
structures for the biased nearest neighbour search problem are possible.

\chapter*{Acknowledgements}

I would like to thank my supervisor Dr. Pat Morin for his willingness to take on
a part-time student and his advice and patience over the past four years.

Thank you April, Deirdre and Tamsin for your love and support.

\afterpreface

\chapter{Introduction}

\section{Problem Statement}

We investigate the utility of data structures for biased nearest neighbour
search in practice. The nearest neighbour search problem is to return the
closest point in a set of points to a specified query point. In the biased
version of this problem, information on the relative frequency of queries
is available, allowing for more common queries to be answered more quickly.

Nearest neighbour search is of considerable practical interest with
applications in geographical information systems, computer graphics,
computer vision, signal processing and machine learning, and has received
attention from both theoretical and applied computer scientists since at least
the early 1970s.

Although theoretically optimal solutions have existed since the 1990s, these
suffer from the so-called ``curse of dimensionality'' where as the number of
dimensions increases, an increasingly large number of points must be searched
until performance in practice is no better than a brute force search of all of
the points. Even in low dimensions, nearest neighbour search can be a difficult
problem in practice given a large enough point set or the requirement to support
a large number of queries in a timely fashion. For these reasons, research has
continued to investigate heuristic and approximate approaches to nearest
neighbour search. 
 
Recently, two groups of researchers \cite{oddson,chan} have proposed a
a biased data structure, called odds-on trees in \cite{oddson} that can be
applied to a variety of geometrical search problems and seems as though it
may be of practical as well as of theoretical interest.

In a sense, the odds-on tree data structure provides a theoretical basis for
the development of heuristics to accelerate geometrical search problems. Since
nearest neighbour search is a problem of practical interest with a number of
existing heuristic solutions with varying degrees of theoretical justification,
it is a natural area in which to apply the odds-on tree with a view to
determining its effectiveness in practice.

Our goal is to implement a practical data structure based on the theoretical
results about odds-on trees.  This involves design and implementation work as
well as careful experimentation to establish correctness and performance
characteristics.

\section{Definitions}

Following terminology associated with Voronoi diagrams, we define the set of
points for which nearest neighbour searches will be performed as sites. Each
site defines an area, called a cell, that for all points inside the cell, the
associated site is the nearest neighbour for a given metric.

Given a point \(p\) its nearest neighbour is the site that is closest to it
for a given metric. Its $k$-nearest neighbours are the $k$ sites that are
closest to it under a given a metric for a given $k$.  A nearest neighbour
search is given a query point \(p\) determine its nearest neighbour.
A \(k\)-nearest neighbour search is given a query point \(p\) return the
$k$-nearest neighbours.

A \(1 + \epsilon\) approximate nearest neighbour search is given a point \(p\),
return a site that is no more than \(1 + \epsilon\) times further away from $p$
than the true nearest neighbour.

A \(1 + \epsilon\) approximate \(k\)-nearest neighbour search is given a point
\(p\), return \(k\) points such than none are further away than \(1 + \epsilon\)
times the true \(k\)th nearest neighbour of $p$.

Entropy is the measure (in bits) of the amount of randomness of a set.  That is,
if \(p_i\) is the probability of the $i$th member of the set, then the entropy
\(H\) is defined as
$$
H = \sum{ p_i \log{1 \over p_i}}
$$

A simplex is the intersection of at most \(d + 1\) halfspaces. A triangle is an
example of a simplex in two dimensions, and a tetrahedron is an example of a
three dimensional simplex.

A decision tree model is a model of computation where the progress of the
algorithm is viewed as decision tree. At each step of an algorithm, a decision
is made as to what operation to perform next, that can be viewed as a branch
in a tree. The child node chosen is determined according to a given criteria.

A comparison tree model is a decision tree where the child is chosen based
upon comparing coordinate values to each other using a suitable ordering.
The comparison tree model is important to geometric search problems as it is
the basis of most implemented data structures due to the fact comparison
avoids problems associated with geometric and numerical robustness.

A linear decision tree model is a decision tree where the child is chosen based
upon a linear function of the point coordinates.

\section{Contribution}

In this thesis we present experimental results demonstrating that distribution
sensitive data structures such as the odds-on tree can be used to develop
data structures that have performance advantages over standard data structures
if provided a suitable probability distribution over the searches.

\section{Organization of the Thesis}

The remainder of the thesis is organized as follows: Chapter 2 summarizes
previous work for biased search, distribution sensitive data structures and
nearest neighbour search.  Chapter 3 describes the design and implementation of
the odds-on tree data structure for nearest neighbour search, including a
description of alternative designs that were explored. Chapter 4 gives details
on the experimentation performed and results obtained, and Chapter 5 summarizes
the results and discusses potential future work.

\chapter{Previous Work}

In this section we examine previous work relevant to odds-on trees. In the first
section we examine data structures for nearest-neighbour search. In the second
section we present results on biased search and describe the odds-on tree data
structure.

\section{Nearest Neighbour Search}

In this section we examine data structures and algorithms for nearest-neighbour
search. A very large number of data structures exist for this problem, so we
we restrict ourselves to algorithms that operate in a comparison tree
model, where at each step data structures make decisions based upon making
comparisons between point coordinates.

Comparison based data structures are easy to implement and fast in practice, at
least for a small number of dimensions, and avoid numerical and geometrical
robustness problems. Evidence suggests that in problems with 16 dimensions or
more, using comparison based approaches performs no better on average than a
brute force search of all of the sites \cite{fastvector}.

Since we do not use the results in the remainder of the thesis, we do not
consider non-comparison based approaches such as such as locality sensitive
hashing \cite{lsh} or metric trees, where points are sorted based upon distance
to a reference point rather than based upon their coordinates. Chapter 4 of the
book by Samet \cite{samet} features a comprehensive treatment of nearest
neighbour search in higher dimensions. The paper by Liu et al. evaluates some of
these techniques experimentally \cite{practicalann}. 

We also restrict ourselves to an in-memory model and so do not consider
external memory based data structures such as R-trees \cite{rtree}. Since an
odds-on tree is essentially a cache, it is less interesting to consider an
external memory representation, although an odds-on tree may be quite useful
as an in-memory cache for an external memory data structure.

\subsection{Voronoi Diagrams}

One way of looking at nearest neighbour search is as a subdivision of the plane
into disjoint areas based upon which site to which they are closest. Finding
the nearest neighbour to a query point is then a matter of determining which
subdivision the query point lies in.

A traditional name for this problem is the post office problem, where given a
set of sites (the post offices) determine for a given query point which
post-office is closest \cite{dutch}.

The Voronoi diagram for a set of sites is the subdivision of the plane (or
hyperplane in higher dimensions) into cells, where for every point in a cell,
its nearest neighbour is the site associated with the cell. Efficient algorithms
exist for creating Voronoi diagrams in low dimensions:

\begin{thm} \emph{(Voronoi Diagram Construction)}
The Voronoi diagram for a set of $n$ points in two dimensions can be computed
in time \(O(n \log n)\). In higher dimensions, the time becomes
\(O(n \log n + n^r{\lceil(d/2)\rceil})\) where $d$ is the dimensionality of the
space \cite{dutch}.
\end{thm}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/voronoi.eps}
\caption{Voronoi Diagram}
\label{fig:voronoi_diagram}
\end{center}
\end{figure}

\subsubsection{Nearest Neighbour Search in a Voronoi Diagram}

Once the Voronoi diagram is constructed, determining the nearest neighbour can
be done by performing point location to determine in which Voronoi cell the
query point lies.

In 2D, an efficient data structure for the more general problem of point
location in a planar subdivision was described by Kirkpatrick
\cite{kirkpatrick}. It based upon first triangulating the subdivision and then
removing independent vertices (those which do not share an edge) from the
triangulation to create a triangulation with fewer faces. Provided a constant
number of vertices are removed from the triangulation at each step, performing
the triangulation recursively will result in a data structure that supports
point location in logarithmic time.

\begin{thm} \emph{(Point Location in the Plane)}
A data structure exists using \(O(n)\) storage that allows for planar
point location to be performed in \(O(\log n)\) time \cite{kirkpatrick}.
\end{thm}

The same bounds were achieved using different data structures by Sarnak and
Tarjan \cite{sarnak} and Edelsbrunner et al. \cite{edelsbrunner}.

In higher dimensions, given $n$ hyperplanes, it is possible to solve this problem
by constructing a \(1/r\)-cutting, that is a collection of simplices that
covers the space and is constructed such that each simplex intersects with no
more than \(n/r\) hyperplanes. Chazelle gives a hierarchical construction
procedure for these hyperplanes such that a simplex is eventually fully
contained in one of the cells, that can then be labelled and used to perform
point location \cite{chazelle}.

\begin{thm} \emph{(Point Location in Higher Dimensions)}
A data structure exists using \(O(n^d)\) storage that allows for  
point location in a d-dimensional arrangement of hyperplanes to be performed in
\(O(\log n)\) time \cite{chazelle}.
\end{thm}

Nearest-neighbour search can be performed using Voronoi diagrams and provides
efficient algorithms in the plane. To extend this to reporting k-nearest
neighbours, once the Voronoi cell containing the query point has been
identified, the neighbour cells of the Voronoi diagram can be examined until
a sufficient number of neighbours has been found. 

In higher dimensions, the time requirement to build a Voronoi diagram and the
space requirement for the point location data structure both depend upon the
dimension of the space in an exponential fashion, which limits both their
theoretical and practical utility for this problem.

\subsection{Quadtrees}

Quadtrees are based upon recursively dividing n-dimensional space into \(2^n\)
equal sized hypercubes, with each node in the tree keeping pointers to its
children.  In the two dimensional case, this means that space is divided into
four quadrants, hence the name quadtrees. Quadtrees were introduced in 1974 by
Finkel and Bentley \cite{quadtree}. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{diagrams/quadtree.eps}
\caption{Quadtree}
\label{fig:quadtree}
\end{center}
\end{figure}

Quadtrees can be built over a region, in which case the subdivision proceeds
until the portion of the region covered by a hypercube has the same value for
some property of interest. For example, given an input image, a region based
quadtree would continue to subdivide until each square covered an area that 
had the same colour. Quadtrees over a region are called Trie-based quadtrees in
\cite{samet} and can be used to approximate Voronoi diagrams.

In this section we use quadtrees that are defined over a set of points, in which
case the recursive subdivision is done until each hypercube contains a single
point from the point set.

The root of the quadtree is the hypercube containing all of the points, a leaf
is a hypercube containing a single point, and the height of a quadtree
is the longest path from the root of the quadtree to the leaf of a quadtree
\cite{dutch}.

\begin{thm} \emph{(Height of a Quadtree)}
The height of a quadtree for a set of $n$ points has an upper bound of \(\log(s/c)
+ 3/2\), where c is the smallest distance between any two points in the set, and
s is the side length of the initial square containing all of the points.
\cite{dutch}.
\end{thm}

Note that the height of the quadtree is not bounded by $n$.

\begin{thm} \emph{(Quadtree Construction)}
A quadtree of height h containing $n$ points can be constructed in \(O((h + 1)n)\)
time \cite{dutch}.
\end{thm}

\subsubsection{Point Location in a Quadtree}

Point location in a quadtree is performed by recursively visiting nodes
starting with the root node. At each step, the child node containing the query
point is visited, and the algorithm terminates when a leaf node is reached.

\begin{thm} \emph{(Point Location in a Quadtree)} 
Point location in a quadtree of height h can be performed in time \(O(h)\). 
\end{thm}

\subsubsection{Nearest Neighbour Search in a Quadtree}

Given a query point $p$ the quadtree node containing $p$, if any, can be found by
starting at the root, comparing $p$ to the bounds of the each child of the root,
and recursively visiting the child containing $p$ until a leaf node is found.  
Since the maximum depth of a quadtree with $n$ points is not bounded by $n$, the
running time of this search algorithm is also not bounded by $n$. This algorithm,
along with a means of performing region queries was described in
\cite{quadtree}.

To perform nearest neighbour search in a quadtree we adapt the procedure given
in \cite{samet} for kd-trees. Finding a nearest neighbour in a quadtree is done
by maintaining a priority queue of nodes to visit, that initially contains the
root node. The algorithm proceeds by removing a node from the priority queue and
examining each of its children. If the child node contains a point it is added
to the result set.

Otherwise, the distance from the query point to each child node is calculated
and if it is smaller than the distance to the current furthest result point,
it is added to the priority queue. The work done by this algorithm is at
least the same amount of work required to do perform a point location query.

\begin{thm} \emph{(Nearest Neighbour Search in a Quadtree)} 
Nearest neighbour search in a quadtree of height h requires \(O(h)\) time.
\end{thm}

\subsection{Compressed Quadtrees}

In a compressed quadtree only nodes that contain a point or have at least two
child nodes are present in the tree. Compressed quadtrees were first described
in \cite{compressedquadtree}, and a detailed description can be found in
\cite{skipquadtree}.

A compressed quadtree can be created by pruning nodes from an existing quadtree
as follows:

\begin{enumerate}
\item If a node contains no children, remove it.
\item If a node has a single child node, change the pointer to the node in the
node's parent to point to the node's child.
\end{enumerate}

Since this procedure requires first building a regular quadtree, it also
requires \(O((h + 1)n\) time to complete.

This construction procedure can be made more efficient, or at least bounded by
n, by first sorting the points according to Morton (or z-) order. Morton order
is defined by interleaving the bits of each of coordinate of the point, such
that the first bit of the first coordinate is followed by the first bit of the
second coordinate and so on until one bit has been used from each coordinate.
This is then followed by the second bit of the first coordinate \cite{morton}.
When performed in two dimensions, this results in a z-shaped curve. This
interleaving of bits is called the ``shuffle'' of a point in \cite{bern}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{diagrams/zorder.eps}
\caption{Z-Order}
\label{fig:z_order}
\end{center}
\end{figure}

The points are then sorted according their Morton order. Each two adjacent
points in the sorted order form a node in the quadtree that can then be
nested by finding the first larger square to the left and the right in the
sorted order \cite{bern}.

\begin{thm} \emph{(Compressed Quadtree Construction)}
A quadtree containing $n$ points can be constructed in \(O(n \log n)\) time
\cite{bern}.
\end{thm}

Chan \cite{chan} showed that the shuffle can be calculated without explicitly
interleaving the bits, and Connor and Kumar \cite{connor} showed that this can
be extended from integer values to IEEE-754 floating point values.

\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{diagrams/compressed_quadtree.eps}
\caption{Compressed Quadtree}
\label{fig:compressed_quadtree}
\end{center}
\end{figure}

\begin{thm} \emph{(Height of Compressed Quadtree)}
The height of a compressed quadtree with $n$ points is \(O(n)\).
\end{thm}

\subsubsection{Point Location in a Compressed Quadtree}

Point location in a compressed quadtree is done by following the same algorithm
as for standard quadtrees. Since the height of a compressed quadtree is
bounded by the number of points, we can bound the time required to perform
point location.

\begin{thm} \emph{(Point Location in a Compressed Quadtree)} 
Point location in a compressed quadtree with $n$ points can be performed in
time \(O(n)\). 
\end{thm}

\subsubsection{Nearest Neighbour Search in a Compressed Quadtree}

Nearest neighbour search in a compressed quadtree can be done by following the
same algorithm as for standard quadtrees. Again, since the height of a compressed
quadtree is bounded by the number of points, we can bound the time required to
perform point location.

\begin{thm} \emph{(Nearest Neighbour Search in a Compressed Quadtree)} 
Nearest neighbour search in a compressed quadtree with $n$ points requires
\(O(n)\) time.
\end{thm}

\subsection{Skip Quadtrees}

Skip quadtrees extend the idea of skip lists \cite{skiplist} to two and higher
dimensional spaces. In a skip list multiple levels of linked lists exist, with
pointers between the levels. The bottom level contains all of the nodes, the
top layer is empty, and (in the randomized version of skip lists,) a node
present in a given layer is present in the layer above it with specified
probability. Corresponding nodes on different levels are then linked. The
expected number of layers is logarithmic, allowing for search in \(O(\log n)\)
time.

Skip quadtrees use essentially the same idea, except each level is a
compressed quadtree rather than a linked list, and terminal nodes in the
compressed quadtree at one level are linked to corresponding nodes in the
lower level tree. We summarize the results are for the randomized version of
skip quadtrees. Deterministic results can also be derived using deterministic
skip lists \cite{skipquadtree}.

Since each level of a skip quadtree consists of a compressed quadtree with
approximately half as many points as that in the level below, we expect to
create \(O(\log n)\) compressed quadtrees.

\begin{thm} \emph{(Skip Quadtree Construction)}
A skip quadtree containing $n$ points can be constructed in \(O(n \log^2 n)\)
expected time.
\end{thm}

\subsubsection{Point Location in a Skip Quadtree}

Point location in a skip quadtree is performed by recursively visiting each level
of the quadtree. Since each level is a compressed quadtree, point location is
first performed in that compressed quadtree. Once the node in the compressed
quadtree is located, the link to the corresponding node in the lower level
is visited until the bottom level is reached and the node in that compressed
quadtree is returned as the result of the point location query. The expected
amount of work performed in each level is constant \cite{skipquadtree}.

\begin{thm} \emph{(Point Location in a Skip Quadtree)} 
Point location in a skip quadtree with $n$ points can be performed in time
\(O(\log n\)) \cite{skipquadtree}. 
\end{thm}

\subsubsection{Nearest Neighbour Search in a Skip Quadtree}

Nearest neighbour search in a skip quadtree is similar to the procedure for
searching a standard quadtree, but the skip structure is used to limit
the number of nodes visited.

For a node \(p\) removed from the priority queue, the skip structure is used to
determine the node \(q\) that is the smallest node in the lowest skip level
that is equidistant to the query point. It is shown in \cite{skipquadtree}
that at most \(O(\log \frac{1}{\epsilon})\) nodes on the path from \(p\) to \(q\)
need to be added to the priority queue.

\begin{thm} \emph{(Nearest Neighbour Search in a Quadtree)} 
Approximate \(1 + \epsilon\) nearest neighbour search for d dimensions in a skip
quadtree with $n$ points requires
\(O(\epsilon^{1 - d}(\log n + \log \epsilon^(-1)))\) expected time.
\end{thm}

\subsection{Kd-Trees}

Kd-trees were created by Jon Bentley in 1975 as an improvement on quadtrees
\cite{kdtree}. Kd-trees are binary search trees for which each non-leaf node
divides the point set in half using an axis-aligned line.

Kd-trees are constructed by choosing an axis, determining the median value in
the point set for the corresponding coordinate, and dividing the points in two
halves using the median value. The two child nodes are then built by
recursively building kd-trees for each half. In the original paper, Bentley
specifies that the axis to split upon is chosen round robin, level by level,
but it is possible to develop heuristics that potentially give better splits
for specific problems.

\begin{thm} \emph{(Height of a Kd-Tree)}
The height of a kd-tree with $n$ points is \(O(\log n)\).
\end{thm}

\begin{figure}
\begin{center}
\includegraphics[scale=0.35]{diagrams/kdtree.eps}
\caption{Kd-Tree}
\label{fig:kd_tree}
\end{center}
\end{figure}

\subsubsection{Point Location in a Kd-Tree}

Point location in a kd-tree can be performed by recursively visiting nodes and
comparing the appropriate coordinate of the point to the splitting axis used
to split the children for that node, and the visiting the appropriate child
node \cite{kdtree}.

\begin{thm} \emph{(Point Location in a Kd-Tree)} 
Point location in a skip quadtree with $n$ points can be performed in time
\(O(\log n\)). 
\end{thm}

\subsubsection{Nearest Neighbour Search in a Kd-tree}

An algorithm for nearest-neighbour search in a kd-tree tree was described
by Friedman et al. \cite{friedman} in 1977 as an improvement on the algorithm
contained in the original paper describing kd-trees.

The algorithm proceeds in a manner similar to the procedure for point-location,
but at each step the child node that was not visited is added to a priority
queue. When a leaf node is reached, the point at that node is added to the
result set and the priority queue is examined. If the closest node in the
priority queue is closer than the furthest point in the result set, then it
is visited according to the recursive procedure given for point location. 

For a \(1 + \epsilon\) approximate nearest neighbour search, nodes in the
priority queue are only visited if they closer than \(1 + \epsilon\) times
the distance to furthest point in the result set.

\begin{thm} \emph{(Nearest Neighbour Search in a Kd-tree)} 
The expected time for nearest-neighbour search in a kd-tree with $n$ points is
\(O \log(n)\), and the worst case is \(O(n)\) \cite{friedman}.
\end{thm}

\subsection{Balanced Box Decomposition (BBD) Trees}

A balanced box decomposition tree is a data structure invented by Arya et al.
\cite{optimalann} that combines advantages of both quadtrees and kd-trees.
Like a kd-tree, as one traverses the tree, the number of points decreases
exponentially, and like a quadtree, the area also decreases exponentially. By
combining these properties, an optimal data structure for approximate nearest
neighbour search is obtained.

Each node in a BBD-tree covers a cell that is the set theoretic difference
between an outer box and an optional inner box. Both boxes are axis-aligned.
If present, the inner box must either touch the edge of the outer box, or be
separated from each edge by at least the width of the box in the direction of
the edge. Each node is an BBD-tree has a bounded aspect ratio that ensures
that the ratio of the shortest edge to the longest edge is bounded by some
constant.

A BBD-tree is constructed through a sequence of two operations, fair-splits
and shrinks. A fair-split is a partition of a box using an axis aligned
hyperplane that maintains the aspect ratio bound but does not necessarily
result in an even partition of points. A shrink operation divides a node into
inner and outer boxes that is used in a manner similar to a compressed
quadtree and can ensure an even partition of points \cite{optimalann}.

Both fair-splits and shrinks can be performed in time linear in the number of
points contained in the cell. A simple strategy for partitioning the points is
the midpoint algorithm that performs a fair-split by partitioning the node
with a hyperplane orthogonal to the longest edge of the outer box. A shrink
operation is somewhat more complicated, potentially requiring two shrink nodes
and one split node to ensure that no child contains more than \(2/3\) of the
points covered by the parent node \cite{optimalann}.

To prove properties of the BBD-tree, Arya et al. assume that fair-splits and
shrinks are applied alternately as the tree is constructed. In practice, they
note that for almost all cases, only fair-splits are required \cite{optimalann}. 

\begin{thm} \emph{(BBD-Tree Construction)} 
A BBD-tree for a \(d\)-dimensional space has \(O(n)\) nodes and can be build in
\(O(d n \log n)\) time \cite{optimalann}.
\end{thm}

\subsubsection{Nearest Neighbour Search in a BBD Tree}

Nearest neighbour search in a BBD Tree is performed in a manner similar to
search in a kd-tree. A priority queue is maintained of nodes to visit that 
initially contains the root node. At each step of the search, the closest
node to the point is removed from the queue and is recursively searched for
the leaf closest to the search point. The distance to the other child nodes
encountered in the search is calculated and they are enqueued in the priority
queue. The search continues until the distance to the node at the top of the
priority queue exceeds the distance to the candidate nearest neighbour. This
can be easily extended to find k nearest neighbours, and to find approximate
nearest neighbours rather than an exact ones.

\begin{thm} \emph{(Nearest Neighbour Search in a BBD Tree)} 
Given a constant \(c_{d,\epsilon} \le \lceil 1 + {6d \over \epsilon} \rceil^d\)
where d is the dimensionality of the space and \(\epsilon\) is the approximation
factor, a \((1 + \epsilon)\)-nearest neighbour search in a BBD Tree can be
performed in time \(O(c_{d,\epsilon} \log n)\) time and k approximate nearest
neighbours can be found in \(O((c_{d,\epsilon} + kd) \log n)\) time.
\end{thm}

\subsection{Approximate Voronoi Diagrams}

For approximate nearest-neighbour searches, it is possible to improve on the
space requirements by creating an approximation to the Voronoi diagram, for
instance using quadtrees \cite{avd}. The complexity of the calculation is
reduced by constructing the Voronoi diagram such that for each cell, any point
within it is a \(1 + \epsilon\)-nearest neighbour, rather than an exact
nearest neighbour.

Har-Peled \cite{avd} provides an algorithm for building an approximate
Voronoi diagram that answers nearest-neighbour searches. His construction is
complicated, requiring first clustering the input sites, then
building a data structure for answering point location in equal ball queries
(a technique also used in locality sensitive hashing \cite{lsh}) and finally
using this data structure to build the approximate Voronoi diagram using
compressed quadtrees.

Arya et al. \cite{arya-avd} extend this to \(t, \epsilon\)-AVD where each
cell in the approximate Voronoi diagram stores \(t\) approximate nearest
neighbours. Their construction makes use of a well-separated pair
decomposition of the sites of the Voronoi diagram. Two sets of points are well
separated if each set can be bounded by a ball of radius \(r\) such that the
distance between the center of each set is at least \(2r\). By decomposing
the sites into well-separated subsets, a set of quadtree nodes can be
generated that are then stored in a BBD-tree. 

We do not describe the construction algorithm in detail as these results are
not used in the remainder of the thesis because we only consider exact
nearest neighbour search. We note that both \(\epsilon\) and t need to be fixed
at construction time, which is less flexible than using quadtrees or kd-trees
directly.

\begin{thm} \emph{(Nearest Neighbour Search in a Approximate Voronoi Diagram)} 
An approximate Voronoi diagram of size \(O({n \over \epsilon ^ d})\) can be
constructed that can answer \((1 + \epsilon)\) approximate nearest neighbour
searches in time \(O(\log {n \over \epsilon})\) \cite{arya-avd}.
\end{thm}

\section{Biased Search}

Biased search is a generalization of search where rather than assuming each
search result is equally likely a probability distribution over the results is
available.

We begin by describing some results from information theory that provide a
bound on how well a biased data structure can perform. We then describe some
classical results for biased search in a single dimension, and then proceed
to some more recent results for higher dimensional search.

\subsection{Information Theory}

Lower bounds for search depend upon some results from information theory that
are briefly summarized here. Information theory is the study of the fundamental
properties of communication systems. A data structure can be seen as a form of
communication system, since it provides a means of communicating data from the
present to the future.  A data structure that allows for efficient search is
essentially solving the same problem as creating an efficient code with which
to transmit a message.

Based upon earlier work by Ralph Hartley, Claude Shannon \cite{claudeshannonwasagod}
defined entropy as the measure (in bits) of the amount of randomness of a set.
That is if \(p_i\) is the probability of the i-th member of the set, then the
entropy \(H\) is defined as
$$
H = \sum{ p_i \log{1 \over p_i}}
$$

Shannon also proved that the efficiency of any communication system is limited
by the entropy of the set of messages to be transmitted.

\begin{thm} \emph{(Fundamental Theorem for a Noiseless Channel)} 
Given a source with entropy \(H\) (in bits) and a channel with capacity \(C\)
(in bits per second), then it is possible to encode a source in such a way as to
transmit at an average rate of \({C \over H} - \epsilon\) symbols per second
where \(\epsilon\) is arbitrarily small, and it is not possible to transmit
at an average rate greater than \(C \over H\) \cite{claudeshannonwasagod}.
\end{thm}

A data structure can be viewed as a communication system that communicates
through time rather than space, by allowing information to be retrieved in the
future. For instance, take querying a binary search tree for a set of keys. If
at each node, a ``0'' or ``1'' is generated depending upon whether the left or
right subtree is taken, then the generated sequence is a code for the key, and
we can apply Shannon's theorem to give a bound on the expected number of
comparisons required.

For random queries for which the data structure has no information other than
the distribution of outcomes, so that \(p_i\) is the probability of searching for
the \(i\)-th key, then any data structure for search can not have an expected
number of comparisons less than \(H\).

Since search time depends upon the expected number of comparisons, the expected
search time is bounded by the entropy, that is \(\Omega(H)\) is a lower bound on
the search time for any data structure.

These results provide bound on how well a biased data structure can perform. The
following sections describe data structures that can meet this bound.

\subsection{Biased Search Trees}

A biased search tree is a generalization of binary search trees that make use
of knowledge of the probability distribution of the search keys to balance the
tree.

The initial work on biased search trees focused on the static case, where
the query frequencies are known in advance and no deletions and insertions
are allowed. This work is summarized in Volume 3 of Donald Knuth's The Art of
Computer Programming \cite{knuth}.

Given a set of $n$ keys \(K_i\) and \(2n + 1\) probabilities
\(p_1, p_2, ..., p_n\) and \(q_1, q_2, ..., q_n\) where \(p_i\) is the
probability that \(K_i\) is the search item and \(q_i\) is the
probability that the search item lies between \(K_i\) and \(K_{i+1}\), an
optimal biased search tree is one that minimizes the expected number of
comparisons to search the tree \cite{knuth}.

Each subtree of a optimal biased search tree is itself optimal, and so the
problem can be solved through a dynamic programming algorithm, using the
following relation: Define \( c(i,j)\) as the cost of an optimum subtree with
weights \((p_{i+1}, ..., p_j; q_{i+1}, ..., q_j;)\) and \(w(i, j)\) be the
sum of the weights, i.e. \(p_{i+1} + ... + p_j + q_{i + 1} + ... + q_j \)
\cite{knuth}.

Then we have that:

\begin{equation}
    \begin{aligned}
    c(i, i) & = 0 \\
    c(i, j) & = w(i, j) + \min_{1<k<j} (c(i, k - 1) + c(k, j)), \text{for } i<j, \\
    \end{aligned}
\end{equation}

This recurrence allows us to find all optimal one node trees and then use those
to find all optimal two node trees, and so forth, until an optimal search
tree has been determined. This minimization is applied to \({1\over{6}} n^{3} \)
values of \( k \), but a monotonicity property allows the running time to be
reduced to \( O(n^2) \) \cite{knuth}.

\begin{thm} \emph{(Construction of an Optimal Static Biased Binary Search Tree)} 
An optimal static biased search tree can be constructed in time \(O(n^2)\) and
space \(O(n^2)\) using a dynamic programming approach \cite{knuth}.
\end{thm}

Knuth also proposed two heuristics that run in \(O(n \log n)\) time and
\(O(n)\) space that produce nearly optimal trees:

Rule I: Use the most frequent item as the root of the tree and apply the same
rule to the subtrees.

Rule II: Choose as the root the item that results in the weight of the left
and right subtrees being as close to equal as possible and apply the same
rule to the subtrees. 

Mehlhorn \cite{mehlhorn} analyzed the results of applying these two rules. He
provides a simple example where Rule I will produce a tree with expected
search time \(\Theta(n)\).

His analysis of Rule II shows that the expected search time for item in the tree
is bounded from above by the entropy \(H\) of the query frequency distribution.
If $P_{opt}$ is the weighted path length of an optimum search tree, and
$P_{balanced}$ the weighted path length of a balanced search tree constructed
according to Rule II, then

$$
{\log(3)}^{-1} \cdot H \le P_{opt} \le P_{balanced} \le 2 + {(1 - \log(\sqrt 5 - 1))}^{-1} \cdot H
$$

\begin{thm} \emph{(Nearly Optimal Static Biased Binary Search Tree)} 
A static biased search tree where the search time for an item is bounded by
the entropy \(H\) of the frequency distribution can be constructed in time
\(O(n \log n)\) and space \(O(n)\) \cite{mehlhorn}.
\end{thm}

Biased search trees allowing insertions and deletions based upon 2-4 and
red-black trees were described by Bent, Sleator and Tarjan \cite{bst} in 1984.
As with 2-4 trees, their data structure relies upon maintaing invariants
on the tree during updates through complicated, multi-case rules for
re-parenting nodes to maintain balance. In this case, the balance condition
depends upon the weight of the node. If all of the weights are one, the
resulting tree is simply a 2-4 tree. Somewhat simpler trees are provided
by Feigenbaum and Tarjan \cite{bst2}. The construction of both data structures
requires the consideration of a large number of subcases to maintain balance and
so we do not describe them further here.

\begin{thm} \emph{(Construction of a Dynamic Biased Binary Search Tree)} 
A dynamic biased search tree can be constructed in time \(O(n)\) that can
perform searches in time \(O(\log(w))\) where \(w\) is the weight of the item
searched for and insertions and deletions in amortized time \(O(W / w)\)
where \(W\) is the total weight of all items in the search tree and \(w\) is
the weight of the item added or removed \cite{bst2}.
\end{thm}

\subsection{Splay Trees}

An alternative approach to biased search is to use data structures that 
modify themselves in response to queries. In this case, rather than specifying
the probabilities in advance, the data structure will change itself to match
the query distribution. 

Splay trees, which were invented by Sleator and Tarjan \cite{splaytree}, are a
self-modifying binary search tree that perform well in practice. They operate
by moving the most recently accessed node to the root of the tree using a
process called splaying that rotates items in pairs according to three cases
that depend upon the structure of the tree.

Although splaying can be costly for an individual access, in an amortized
sense, operations on a splay tree take \(O(\log n)\) time.

\begin{thm} \emph{(Splay Trees)} 
Insertion, deletion and search in a splay tree take \(O(\log n)\) time in
an amortized sense \cite{splaytree}.
\end{thm}

Sleator and Tarjan also showed that splay trees perform within a constant
factor of any static optimal search tree over a sequence of accesses, and
famously, conjectured that splay trees also perform within a constant factor
of any dynamic search tree as well.

\begin{thm} \emph{(Static Optimality of Splay Trees)} 
The total amortized time required for a sequence of $m$ accesses to a $n$
node splay tree is \(O(n \log n + m)\) \cite{splaytree}.
\end{thm}

It is possible to create biased versions of other, simpler data structures
such as randomized search trees (treaps) \cite{treap} and skip lists
\cite{bsl2} but these do not provide better theoretical performance guarantees
than biased search trees, and do not seem to provide better performance in
practice than splay trees.

\subsection{Planar Point Location}

Looking at biased search in higher dimensions, we first consider the planar
point location problem: given a subdivision of the plane and a query point \(p\),
construct a data structure that allows us to efficiently determine which face
of the plane contains \(p\). If all of the planar subdivisions and the query
point lie on a single line then this problem is the standard one dimensional
search problem and the appropriate data structure is a binary search tree.

We discussed the non-biased version of this problem above in the section on
nearest-neighbour search in a Voronoi diagram, and noted that a linear space
and \(O(\log n)\) search time solution existed for this problem.

In the biased case, as with binary search trees, if \(p_i\) is the probability
that the answer to a query is in the \(i\)th subdivision and we are given a
probability distribution over the subdivisions such that each is chosen with
\(p_i\) then the expected number of comparisons required to locate the query
point \(p\) is bounded by the entropy of the set of subdivisions.

A data structure for biased point location in a triangulation was described
by Iacono \cite{iacono}. It is based on a variation of Kirkpatrick's data
structure for solving point location \cite{kirkpatrick}, which works by
constructed a hierarchical triangulation by selectively removing vertices
from each level of the hierarchy, until the top level contains a single triangle.

In Iacono's version, the vertices chosen to be removed are such that they are
not of large degree and are not incident to a high probability region. Another
approach, by Arya et al. matches these space and time bounds \cite{simpleentropy}.

\begin{thm} \emph{(Distribution Sensitive Point Location in a Triangulation)} 
Given a triangulation, it is possible to perform point location in time
\(O(H + 1)\) and space \(O(n)\) where H is the entropy of the set of triangles
making up the triangulation. 
\end{thm}

\subsection{Biased Range Trees}

Biased range trees are a data structure invented by Dujmovic et al.
\cite{biasedrange} to solve two-sided planar orthogonal range counting problems.
Given a set \(S\) and a query point \(q_x, q_y\), the problem is to count all
points in \(S\) that are above and to the left of the query point
\(q_x, q_y\), that is \(p_x \ge q_x\) and \(p_y \ge q_y\).

This is a restricted case of the orthogonal range counting problem that allows
for a rectangular query area in two or higher dimensions. A data structure
invented by Jon Bentley \cite{rangetree} called a Range Tree can solve range
search problems in linear space and logarithmic time. A one dimensional range
tree is simply a balanced binary search tree, and in \(d\) dimensions, one
coordinate is chosen to create a binary search tree, and each child node
contains a \(d - 1\) range tree. 

\begin{thm} \emph{(Planar Range Search using Range Trees)} 
A range tree for a set of points \(S\) of cardinality \(n\) can be built in
time \(O(n \log n)\) and requires \(O(n \log n)\) space. It can report the
points in a rectangular query area in time \(O(\log^2 n + k)\) time, where
\(k\) is the number of points in the query area. A technique called
fractional cascading can reduce the query time to \(O(\log n + k)\)
\cite{dutch}. 
\end{thm}

Kd-Trees and Quadtrees can also be used to solve the orthogonal range counting
problem, but with \(O(\sqrt n)\) time and \(O(n)\) worst case query time
respectively.

Although the biased range tree solves a fairly restricted problem, it shares
many ideas with odds-on trees and is worth considering in detail. Biased range
trees operate in the comparison tree model of computation, where at each step
of the search decisions are made by comparing coordinate values. This has
advantages in practice because comparisons avoid the numerical and geometrical
robustness problems that can arise by using other operations \cite{biasedrange}. 

The answer to any range counting query is an integer in \({0, ..., n}\). Let
\(p_i\) be the probability that the answer is i. Since we are operating in a
comparison based model, Shannon's theorem for a noiseless channel applies and
the entropy \(H=\sum_{i=0}^n{p_i\log(1/p_i)}\) is a lower-bound on the number of
comparisons required. This is not a strong lower bound, and no comparison (or
decision) tree is able to meet it in general \cite{biasedrange}.

A stronger lower bound for the range counting problem can be found by creating
an arrangement \(A\) of \(2n\) rays originating at each point in the set \(S\),
with one ray facing to the left and one ray facing down. This partitions the
plane into a set of faces \(F(A)\).

Now consider a comparison tree to solve the range counting problem. Each leaf of
the comparison tree is a rectangle because it is based upon comparisons to
coordinates that are axis aligned.  These leaf rectangles do not intersect with
any of the edges of \(A\) because that implies that the answer to the range
counting problem would not be consistent for the entire rectangle. This implies
that we can use \(F(A)\) as a data structure for the range counting problem,
and that Shannon's theorem once again provides a lower bound of:

$$
H=\sum_{f \in F(A)}{p_f\log(1/p_f)}
$$

This lower bound is still not strong enough. To see this consider figure
~\ref{fig:rangelb}. The query points are evenly distributed in the \(n + 1\)
red circles. In this case, the query points are always in the same face of
\(F(A)\), so the entropy above is 0. A comparison tree based data structure
for this problem uses \(n + 1\) leaves to determine which circle contains
the query point, so the the lower bound must be greater than \(\log(n + 1)\).

\begin{figure}
\begin{center}
\includegraphics[scale=0.6]{diagrams/range_counting.eps}
\caption{The range counting lower bound is not always achievable}
\label{fig:rangelb}
\end{center}
\end{figure}

A Biased Range Tree relies upon two data structures. The primary search tree
is based upon a kd-tree. The nodes of the primary tree are associated with
regions of space, and the root node covers the plane. Any node in the primary
search tree that has depth greater than \(O(\log n)\) is called a ``bad'' node.
These regions are potentially open on two sides, since the data structure is
designed to support 2-sided queries \cite{biasedrange}. 

Each child node in the tree is constructed by removing a horizontal (at even
depths) or vertical (at odd depths) strip from region associated with the
parent region. These strips are constructed such that the probability a point
being in the region associated with a child node is at most half the probability
of the point being in the parent region.

Each node also has two associated data structures, called catalogues, that 
contain subsets of \(S\) sorted by the x- and y- coordinates respectively.
Each catalogue is indexed by a biased search tree, and fractional cascading is
used to speed up search within the catalogues.

The second data structure is a backup range tree, as described above, that 
uses \(O(n \log n)\) space and can answer range search queries in time
\(O(\log n)\). This data structure is used to answer queries for which the
depth of corresponding node in the primary tree would exceed \(O(\log n)\),
that is the ``bad'' nodes.

Search within the biased range tree occurs as follows: the primary tree is
first searched to find a node such that the query region defined by \(q_x\) and
\(q_y\) is contained within the region corresponding to the node. If this node
is a ``bad'' node, then the backup range tree is used to answer the query.
Otherwise, the indices are used to locate the node are used to locate \(q_x\)
and \(q_y\) in the catalogues for the node determine the result of the range
counting query, and the tree is walked back to the root, locating the query
point in the catalogues of each node along the way, and adding results to
the range counting query.

\begin{thm} \emph{(Two-Sided Orthogonal Biased Range Search)} 
Two sided orthogonal biased range search can be performed using space
\(O(\log n\)) and expected time \(O(H)\) where \(H\) is the entropy of the
set of possible range counting results (an integer in \({0, ..., n}\)) under
a given probability distribution \(P\). \(H\) is a lower bound on the expected
number of comparisons done by any comparison-based data structure, so
biased range trees are an optimal data structure for two-sided orthogonal
range search \cite{biasedrange}.
\end{thm}

\subsection{Odds-on Trees}

Odds-on trees make use of similar ideas of those described above to solve
2-sided orthogonal range searches, but are capable of solving a large class
of geometric search problems.

Given a probability distribution \(D\) over a set of queries, the odds-on tree
is capable of answering any query drawn from \(D\) in expected time
\(O(H^* + 1)\) where \(H^*\) is a lower bound on the expected cost of any
linear decision tree that solves the search problem. In particular, the
entropy \(H\) of the output \(D\) is one such bound for many search
problems. A linear decision tree is a search tree where the decision of which
child node to follow is based on a linear function.

An Odds-on tree consists of a primary search tree and a backup search tree.
The primary search tree is based upon a data structure called partition trees,
which are an means of dividing \(d\)-dimensional spaces into simplices. A
simplex is the intersection of at most \(d + 1\) halfspaces; a triangle is an
example of a simplex in two dimensions. The simplices in the partition tree
potentially overlap each other.

\begin{thm} \emph{(Partition Trees)} 
Given a set of \(S\) points, for any \(\epsilon > 0\) a data structure called
a partition tree can be constructed that uses \(O(n)\) storage and can answer
counting queries for a search simplex in time \(O(n^{1/2 + \epsilon})\) time and
report those points in an additional \(O(k)\) time, where \(k\) is the number
of points in the search simplex. A partition tree can be constructed in
\(O(n^{1 + \epsilon})\) time \cite{dutch}.
\end{thm}

The points used to create this partition are drawn from the query distribution
\(D\), and the maximum depth of any node in the tree does not exceed a specified
constant. For each node in the partition tree the area corresponding to the node
is searched to see if the answer to the search problem is the same for all points
associated with the area. Since the simplices created by a partition tree are
possibly overlapping, it is necessary to determine the polytope associated with
the node which is not associated with any other node in the tree.

If the result of the search is the same for all points in the polytope, the node
is marked as terminal, any children nodes are removed, and the result of the
search is stored with the node to answer future searches.

The backup tree can be any data structure capable of answering the original
search problem in \(O(\log n)\) time.

Search in the tree occurs as follows. Given a query, the tree is searched until
either a terminal leaf node is reached, in which case the stored result is used
to answer a query, or a non-terminal leaf node is reached, in which case the
backup tree is used to answer the query \cite{oddson}. Essentially the same data
structure with a different analysis was created by Chan et al. \cite{chan}.

\begin{thm} \emph{(Odds-on Tree)}
For any \(n\) and \(\tau>0\) an odds-on tree of size \(m = n^\tau\) of maximum
depth k = \(\lfloor {1 \over 4} \log_{r \over 3} m \rfloor \), where \(r\) is
the number of simplices associated with the root node of the odds-on tree, can
be constructed in time \(O(m \log^{O(1)} n)\) plus the cost of \(O(m)\) samples
from \(D\) plus the cost of \(O(m \log^{O(1)} n)\) searches to determine whether
a node is terminal.

The odds-on tree can be answer searches in expected time \(O(H^* + 1)\) where
\(H^*\) is a lower bound on the expected cost of any linear decision tree
that answers the search \cite{oddson}.  
\end{thm}

\chapter{Implementation}

This chapter describes the implementation of an odds-on tree based data
structure for nearest-neighbour search.

Partition trees were used in the analysis of odds-on trees. Although they have
useful theoretical properties, they are complicated and to our knowledge have
never been implemented and so were not considered as the basis of a practical
implementation.

Compressed quadtrees and kd-trees lack strong theoretical performance
guarantees but they are simple to implement and perform well in practice. In
general, kd-trees have better performance guarantees and out-perform quadtrees,
but we chose to implement both data structures, as the initial results using
kd-trees were not promising and it was possible that quadtrees would have
advantages as the basis for an odds-on tree implementation that outweigh their
disadvantages in general. 

The implementations are in C++. The use of C++ provides great control over
memory allocations and the placement of objects in memory, allowing for
optimizations. Since it is not garbage collected, experimental results can be
more deterministic as the programs will not be interrupted at unpredictable
intervals for garbage collection. C++ also has reasonably performant
implementations of basic data structures such as vectors and lists in the
standard library. The implementations were parameterized using templates to
allow for user defined classes to be used for points, and the underlying data
type used for numbers to be chosen. The experiments are done using double
precision floating point numbers.

The heap data structure implementation in the C++ standard library on the
development platform was found to have poor performance. Two priority queue
implementation were developed based upon the heap data structure, one of which
enforced a queue size limitation for use when searching for \(k\) nearest
neighbours. Both were found to outperform the C++ priority queue implementation
based upon the std::vector class and were used in both the kd-tree and
compressed quadtree implementations.

\section{Kd-tree Implementation}

We first describe our implementation of kd-trees. The kd-tree implementation was
used as the backup oracle for both the kd-tree and quadtree based odds-on tree,
and as the reference implementation of kd-trees for experimentation.

The kd-tree is built as described in chapter two, by choosing an axis round
robin, determining the median value in the point set for the corresponding
coordinate and dividing the points in two halves using the median value. The two
child nodes are then built by recursively building kd-trees for each half.

The most significant optimization made during the implementation is to use
a memory pool rather than the system allocator for allocations. This allowed
for left and right children of a node to be placed at adjacent locations in
memory for better locality of reference and avoided the overhead associated
with many small allocations using the system allocator, which resulted in a
50 percent improvement in running time. The implementation was based upon the
kd-tree optimization notes in \cite{physicallybasedrendering}.

We performed some experiments comparing our kd-tree implementation to the 
kd-tree implementation in Mount and Arya's ANN library\cite{ann}. Our
implementation is slower, but not by a large amount. For instance, running
1000000 queries on a point set of 100000 points, our implementation is eleven
percent slower in two dimensions and eighteen percent slower in three
dimensions. The ANN library is also as the reference implementation in order
to test the correctness of our implementation, by comparing results on sets
of randomly chosen points and searches.

In order to support odds-on tree construction, the construction routines in
the kd-tree were modified to optionally support making a callback function
call as each node is constructed. If the callback returns false, the kd-tree
construction ends without processing all of the points. This callback
mechanism also allows for additional information to be stored at the kd-tree
node as required. 

\section{Kd-Tree Based Odds-on Tree Implementation}

The odds-on tree is built using two point sets. The first point set is used
to build the backup kd-tree that is used to answer queries which can not be
efficiently answered using the odds-on tree and during construction to
answer nearest-neighbour queries while building the cache.

The second point set is drawn from a probability distribution of query
frequencies and is used to build the odds-on tree cache. The callback mechanism
described above is used to determine if each ``corner'' of a newly constructed
node has the same nearest neighbour. If this is the case, the answer to a
nearest neighbour query for any point inside the node would be the same, and it
is marked as terminal and the nearest neighbour is stored with the node.

A maximum depth for a constructed node is also enforced by the callback
function that will end the build process for a node without marking it
terminal if the depth of its subtree will exceed a pre-determined maximum.

The build process continues until one of three conditions occurs: all input nodes
have been processed, all nodes have been marked terminal, or all remaining nodes
exceed the maximum depth specified.

Both the kd-tree and quadtree based odds-on tree implementations handle queries
in the same fashion.

To handle queries, the odds-on tree relies upon a locate procedure that searches
the cache for a terminal node containing the query point. If a terminal node
exists that contains the query point, the stored nearest neighbour is returned.
Otherwise, the backup kd-tree is searched for the nearest neighbour.

\section{Quadtree Based Odds-on Tree Implementation}

The Quadtree based odds-on tree is implemented using compressed quadtrees. Two
approaches for building compressed quadtrees were investigated. The first,
based upon sorting in Morton order did not perform well, and the second, based
upon a top down recursive construction was used in the experimentation.

As described in Chapter Two, sorting points in Morton order are sorted in the
order in which they would be visited during a depth first traversal of a
compressed quadtree. The sorted points were then visited in order to find
sequences of points with a common nearest neighbour. Once a sequence was
discovered, the end points were used to construct a bounding box, and the
``corners'' of the bounding box were tested using a backup kd-tree structure.
If all of the corners shared a common nearest-neighbour, the bounding box is
marked as terminal and added to a list of cache nodes. These cache nodes were
then used to create a bounding volume hierarchy that is used as the cache for
the odds-on tree. This implementation is not competitive with kd-trees and so
was abandoned in favour of a standard top down recursive build procedure.

The recursive, top down build procedure for compressed quadtrees was implemented
as described in Chapter Two. The root node covers the area containing all of the
input points, and each node is split into \(2^d\) children of equal size. A 
a compressed quadtree is then built for each child. The build process terminates
when only a single point is present in the node or the build callback function
indicates that the build should stop. Once the children are built, the child
nodes are examined. If only one child node contains children itself, the parent
node is replaced by this child, which causes a compressed rather than standard
quadtree to be built.

As with the kd-tree implementation, as each node is constructed a callback
function is called that allows for the node to be marked as terminal if each
corner has the same nearest neighbour. A pre-determined maximum depth is also
enforced. Building the compressed quadtree implementation top down is not as
fast as sorting by Morton order, but results in a higher quality cache so that
cache hit rates are higher and the overall performance is better. 

The query procedure is essentially the same as for the kd-tree based
implementation, with a locate procedure attempting to find a terminal node in
the cache that contains the query point. If this fails, the backup kd-tree is
used to answer the search.

\section{Test Harness Implementation}

A test harness for the odds-on tree and kd-tree implementations is used to
read test data and output performance metrics.

A C++ program was written to instantiate each of the odds-on tree implementations
and the kd-tree implementation. The point, search and sample sets are read from
disk to ensure repeatability of results. The results of each search are written
to standard output. This allows for comparison of results to a reference
implementation. Performance metrics are written to standard error. This allows
for search results to be discarded by redirecting standard output to the null
device while retaining the performance metrics. Writing results to the terminal
causes the searches to perform substantially more slowly.

The test harness outputs the time required to build the tree and the time
required to run the queries after the tree is built. The timings are collected
using the clock\_gettime calls to read the system realtime clock under Linux. 

The odds-on tree implementations also output the number of ``cache hits'', that
is the number of searches answered by the odds-on tree rather than the backup
tree.

The C++ test harness was called by a separate python script which given a
directory of sample data, would automatically call the odds-on tree
implementation and kd-tree implementation with the appropriate permutations
sample and search sets for each probability distribution over the search sets
present. It also provides the capability to randomize the order in which the
tests were, validate that the search results match between the kd-tree and the
odds-on tree, and limit the number of runs so that the full set are not
performed.

\chapter{Experimental Results}

This chapter describes the experimentation performed to test the odds-on tree
implementation, including experimental design, performance metrics, test
platform, and presents results from the experimentation.

\section{Experiment Design}

The experimental hypothesis is that the odds-on tree provides better nearest
neighbour search performance than a kd-tree implementation in cases where the
entropy of the point set under a given probability distribution is suitably low.

The following sections describe and justify the experimental parameters chosen
for this study.

\subsection{Dimension}

The dimension of the point sets is varied across 2, 3, 4, and 8 dimensions.
Previous experimental work indicates that dimensions of 16 and above are not
suitable for exact search in kd-trees (and by extension quadtrees) and typically 
approximate nearest neighbour search for good performance \cite{app-ann}. Pilot 
experimentation indicated that even at eight dimensions the benefits of using
odds-on tree are slight and so higher dimensions were not investigated, as
increase in dimension substantially increases running times for both the
odds-on tree and kd-tree implementations.

\subsection{Point Set Size}

The point set is the set of sites that are potential results for a nearest
neighbour search. The point sets are generated by sampling a multi-variate
uniform distribution on the range \([-1, 1)\) for each dimension.

The number of points in the point set is varied across 1000, 5000, 10000, and
50000 points. Larger point set sizes were tried during pilot experimentation but
substantially increased run times without giving significantly different
results.

\subsection{Search Set Entropy}

The search set is the set of query points that are used to test the odds-on
tree and kd-tree implementations.

A multi-variate Gaussian distribution is used to generate the search set. The
mean for each dimension is 0, so the search set is distributed around the
origin. The standard deviations used are 0.5, 0.25, 0.1, 0.05, and 0.01.

The figures below show an example of the point and sample set distributions for
each sigma value. The sample set is a smaller set drawn from the same
distribution as the search set and so illustrates the search set distribution.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/pts_plot_sigma0.01.eps}
\caption{Points and sample distributions (2d, sigma = 0.01)}
\label{fig:points_and_sample_2d_0_01}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/pts_plot_sigma0.05.eps}
\caption{Points and sample distributions (2d, sigma = 0.05)}
\label{fig:points_and_sample_2d_0_05}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/pts_plot_sigma0.10.eps}
\caption{Points and sample distributions (2d, sigma = 0.10)}
\label{fig:points_and_sample_2d_0_10}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/pts_plot_sigma0.25.eps}
\caption{Points and sample distributions (2d, sigma = 0.25)}
\label{fig:points_and_sample_2d_0_25}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/pts_plot_sigma0.50.eps}
\caption{Points and sample distributions (2d, sigma = 0.50)}
\label{fig:points_and_sample_2d_0_50}
\end{center}
\end{figure}

For each case the search set consists of 100000 points. In pilot experiments
this size ensured long enough running times to make meaningful comparisons
between experimental cases without requiring an excessive amount of time to
perform all of the experiments. If too few searches are used the runtimes may
end up being within fractions of a second of each other and so much more likely
to be affected by noise.

The experimentation in this thesis is done using synthetic datasets due to the
difficulty in finding existing datasets that include suitable probability
distributions over the searches . This also makes it difficult to demonstrate
that the search distributions chosen reflect ``real-world'' problems.

Although it is possible to take existing datasets and attempt to derive or
create typical search distributions for them, this would essentially still be
using synthetic data because the primary factor in odds-on tree performance is
the entropy of the sites given a probability distribution over the searches.

\subsection{Sample Set Size}

The sample set is a set of searches used to build the odds-on tree. Each sample
set is created by sampling the same probability distribution used to create the
search set. A different sample set is generated for each run. Example sample
sets are shown in the figures above.

The sample set set size chosen will be $0.5n$, $n$, and $2n$, where
$n$ is the cardinality of the point set. Pilot experimentation showed that
these values provide a reasonable indication of the performance impact of
sample set size. 

\subsection{Maximum Build Depth}

The construction of the odds-on tree is controlled by a parameter that 
determines how ``deep'' the recursive build will proceed before terminating.
This parameter will be set be varied between $\log(n)$, $1.5\log(n)$, and
$2\log(n)$ where $n$ is the cardinality of the sample set. Values less than
$\log(n)$ and greater than $2\log(n)$ did not have a substantial impact on
results in the pilot experimentation. 

\subsection{Number of Runs}

Each experimental configuration is repeated ten times with a different sample
set in each case. During pilot experimentation, this was shown to generate
statistically significant differences between configurations for nearly all cases
examined (t-test, p < 0.0001).

\section{Measuring Performance}

\subsection{Benchmark Platform}

The experimentation is done using a virtualized host provided by a third party.
This allows for easy duplication of hardware to run experiments in parallel and
aids in repeatability. Although virtual instance types are retired by
providers, physical hardware is likewise only available for limited period of
time.

The benchmark platform is a Digital Ocean droplet with a 2.4GHz processor, 1 GB
of RAM, and 30 GB SSD secondary storage. During pilot experimentation, the
digital ocean instance was found to considerably faster and so more
cost-effective than an Amazon Web Services EC2 Medium Compute Optimized
instance, with two 64-bit processors and 1.7GB of RAM. This is likely due to the
Amazon instance type not providing SSD backed storage and so becoming I/O bound 
reading experiment data. 

\subsection{Performance Metric}

The primary performance metric is running time. As mentioned above, the time
required to build the tree and the time required to run the queries are measured
separately, allowing these times to be compared independently of the overall
running time. This is done because the effect of build time can be made
arbitrarily small by increasing the number of queries performed.

A secondary performance metric for the odds-on tree is the cache ``hit rate''.
This is the number of queries answered by the odds-on tree as opposed to the
backup tree. This allows for comparison between the kd-tree based and quadtree
based odds-on tree implementations. 

\section{Results}

This section contains the results and analysis for each experimental
parameter described above and concludes with a comparison of the quadtree and
kd-tree based odds-on tree implementations and their performance relative to
the kd-tree.

\subsection{Dimension}

An increase in dimension caused a corresponding increase in the query time of
the odds-on tree and kd-tree implementations. It also caused the relative
performance of the odds-on tree to become worse. In two or three
dimensions and small search set sigma, the odds-on tree outperforms the kd-tree.
In eight dimensions, even with a favourable search set sigma, the kd-tree
equals or outperforms the odds-on tree in query time. Typical results for a
quadtree based odds-on tree are shown in ~\ref{fig:dimension_ctime}. This
decrease in performance is likely caused by the decreased effectiveness of a
fixed sample size as dimensionality increased.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/dims_pts50000_sample25000_sigma0.01_qtime.eps}
\caption{Query Time (pts = 50000, sample = 25000, sigma = 0.01)}
\label{fig:dimension_ctime}
\end{center}
\end{figure}

Build times likewise became worse for the odds-on tree implementations as
dimensionality increased. The impact on the kd-tree build time was negligible.

\subsection{Point Set Size}

To see the impact of point set size, the build depth and sample sigma are held
constant and the build and construction times are plotted for each point set
size.

For the kd-tree based implementation increasing the point set size increases
build time and query time and query time variance. The kd-tree query time
increases, but the build time is not significantly impacted at the point set
sizes tested, in comparison to the odds-on tree implementation.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/2d_group_bypts_sigma0.01_ctime.eps}
\caption{Construction Time (kd-tree, 2d, sigma = 0.01)}
\label{fig:point_set_size_ctime}
\end{center}
\end{figure}

For the most part the relative query performance of the kd-tree based odds-on
tree and kd-tree did not change with increased point set size. If the odds-on tree
outperformed the kd-tree for small point set sizes, it also did so for large
point set sizes. There were a few exceptions at larger search set entropies and
higher dimensions.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/2d_group_bypts_sigma0.01_qtime.eps}
\caption{Query Time (kd-tree, 2d, sigma = 0.01)}
\label{fig:point_set_size_qtime}
\end{center}
\end{figure}

The results for the quadtree based odds-on tree follow the same pattern, with
increased point set size increasing the overall running time, but not changing
the relative performance of the odds-on tree and the kd-tree. Typical build
time results are shown in ~\ref{fig:point_set_size_ctime_qt} and construction
time results in ~\ref{fig:point_set_size_qtime_qt}  

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/2d_group_bypts_sigma0.01_ctime_qt.eps}
\caption{Construction Time (quadtree, 2d, sigma = 0.01)}
\label{fig:point_set_size_ctime_qt}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/2d_group_bypts_sigma0.01_qtime_qt.eps}
\caption{Query Time (quadtree, 2d, sigma = 0.01)}
\label{fig:point_set_size_qtime_qt}
\end{center}
\end{figure}

\subsection{Search Set Entropy}

Increasing sigma value causes the performance of the odds-on tree to
At small sigma values, the odds-on tree performs significantly better than
the kd-tree. As sigma increases, the odds-on tree begins to perform worse
until its performance is more or less equivalent, indicating that almost
all queries are being handled by the backup kd-tree rather than the odds-on
tree itself.

As sigma increases, the performance of the kd-tree itself also improves.
This is due to the kd-tree doing less backtracking to determine if the
candidate nearest-neighbour is in fact the actual nearest-neighbour at
larger sigma values. For sigma = 0.25, the kd-tree does 17 percent less
backtracking, and for sigma = 0.5 the kd-tree does 35 percent less
backtracking. This is in turn a side effect of the searches being centered
around the origin. As sigma increases, the searches are more likely to
fall on the periphery of the kd-tree, meaning less nodes inside the
kd-tree need to be visited while backtracking to find the nearest
neighbour. Figure ~\ref{fig:sigma_qtime} shows query time results for a 2d
kd-tree based odds-on tree..

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/2d_pts10000_sample5000_qtime.eps}
\caption{Query Time (kd-tree, 2d, pts=10000, sample=5000)}
\label{fig:sigma_qtime}
\end{center}
\end{figure}

The construction time for the kd-tree based odds-on tree implementation worsens
as sigma increases until it reaches a maximum at a sigma of 0.1. At this point,
the construction time begins to decrease. This decrease matches the decrease
in query time for the kd-tree at those sigma values. Since constructing the
odds-on tree requires a large number of kd-tree based nearest neighbour
searches, the improved kd-tree performance also benefits the construction time
for the odds-on tree. Figure ~\ref{fig:sigma_ctime} shows construction time
results for a 2d kd-tree based odds-on tree..

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/2d_pts10000_sample5000_ctime.eps}
\caption{Construction Time (kd-tree, 2d, pts=10000, sample=5000)}
\label{fig:sigma_ctime}
\end{center}
\end{figure}

The observed decrease in performance with increasing sigma values is due to
a larger odds-on tree being built. The number of terminal nodes remains
relatively small while the overall size of the odds-on tree and the number of
nearest neighbour queries required increases substantially. Typical results are
shown in ~\ref{fig:node_counts}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/2d_pts10000_sample10000_builddata.eps}
\caption{Node Counts (kd-tree, 2d, pts=10000, sample=10000, build depth=13)}
\label{fig:node_counts}
\end{center}
\end{figure}

\subsection{Sample Set Size}

The sample set size has a direct impact on build and query times. Increasing
the sample set size requires more time to build due to additional nearest
neighbour queries during the cache construction, but usually results in a
better cache, causing the a corresponding decrease in query time.  An example
of typical build time results are shown in ~\ref{fig:sample_set_size_qtime}
and query time results are in ~\ref{fig:sample_set_size_ctime}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/kt_2d_pts10000_groupbysample_ctime.eps}
\caption{Construction Time(kd-tree, 2d, pts=10000)}
\label{fig:sample_set_size_ctime}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/kt_2d_pts10000_groupbysample_qtime.eps}
\caption{Query Time(kd-tree, 2d, pts=10000)}
\label{fig:sample_set_size_qtime}
\end{center}
\end{figure}

Because changing sample set size is a trade off between build time and query
time, it makes sense to present results for total time. A typical result for
overall running time is shown in ~\ref{fig:sample_set_size_total}.  For lower
sigma values the higher quality cache that results from a larger sample set size
results in a lower overall running time. As sigma increases, this benefit
decreases until performance becomes roughly equivalent for all sample set sizes,
and before finally becoming worse for larger sample set size, at which point the
kd-tree outperforms the odds-on tree.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/kt_2d_pts10000_groupbysample_total.eps}
\caption{Total Time(kd-tree, 2d, pts=10000)}
\label{fig:sample_set_size_total}
\end{center}
\end{figure}

Increasing the sample set size increases both the number of terminal nodes and
the total number of nodes in the odds-on tree as can bee seen in
~\ref{fig:sample_set_size_nodecounts}. The overall ratio of terminal nodes to
total nodes is highly dependent on the search set entropy. At low entropy,
increasing the sample set size increases the number of terminal nodes while
maintaining a small overall odds-on tree size. This means that the odds-on
tree is quick to search, and searching it is likely to result in finding a
terminal node. This is not the case at higher search set entropy values.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/kt_2d_nodecounts_pts10000_groupbysample.eps}
\caption{Node Counts (kd-tree, 2d, pts=10000)}
\label{fig:sample_set_size_nodecounts}
\end{center}
\end{figure}

\subsection{Maximum Build Depth}

The maximum build depth parameter limits how deep the odds-on tree will be
built, which affects both construction time and query time. The smallest build
depth value ($\log(n)$, where $n$ is the point set size) investigated typically
results in faster construction time but slower query times and slower overall
performance.

The results for the other two build depth values ($1.5*\log(n)$, $2*\log(n)$)
are typically very close together.  Example results for build time are shown in
~\ref{fig:max_build_depth_ctime} and for query time are shown in
~\ref{fig:max_build_depth_qtime}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/kt_3d_pts10000_sample5000_ctime.eps}
\caption{Construction Time(kd-tree, 3d, pts=10000, sample=5000)}
\label{fig:max_build_depth_ctime}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/kt_3d_pts10000_sample5000_qtime.eps}
\caption{Query Time(kd-tree, 3d, pts=10000, sample=5000)}
\label{fig:max_build_depth_qtime}
\end{center}
\end{figure}

As with sample set size, build depth is a trade off between build time and query
time. A typical result for overall running time is shown in
~\ref{fig:max_build_depth_total}. In this case the trees with greater depth are
faster for the sigma = 0.01 condition, but the additional build time ends up
making them slower overall for greater sigma values. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/kt_3d_pts10000_sample5000_ctime.eps}
\caption{Total Time(kd-tree, 3d, pts=10000, sample=5000)}
\label{fig:max_build_depth_total}
\end{center}
\end{figure}

In comparison to the sample set size, the maximum build depth parameter has
a much less predictable effect on the performance of the odds-on tree. Based
upon the results here, it seems best to set it at a moderate value, say between
$1.25\log(n)$ and $1.5\log(n)$ and use sample set size to make trade offs
between build time and construction time of the odds-on tree.

For the quadtree based odds-on tree there is a lot of variance in the running
time and the build depth parameter does not seem to have a significant impact
on either build time or construction time. As with the kd-tree based
implementation, it seems best to set it at moderate values and use the sample
set size, which has more predictable impact on performance.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/qt_3d_pts10000_sample5000_ctime.eps}
\caption{Construction Time(quadtree, 3d, pts=10000, sample=5000)}
\label{fig:max_build_depth_ctime_qt}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/qt_3d_pts10000_sample5000_qtime.eps}
\caption{Query Time(quadtree, 3d, pts=10000, sample=5000)}
\label{fig:max_build_depth_qtime_qt}
\end{center}
\end{figure}

\subsection{Quadtree vs. Kd-Tree Based Implementation}

Pilot experimentation did not show either the kd-tree or quadtree based
implementation to be consistently better than the other. In worst case analysis,
kd-trees are expected to exhibit better performance than compressed quadtrees
for nearest neighbour search, but this is not necessarily the case for the
experimental conditions investigated in this thesis.

The quadtree build times are generally worse than the kd-tree based build times.
A kd-tree can be built efficiently by sorting in place, but constructing a
compressed quadtree using a recursive top-down procedure requires allocating
space for new nodes, copying points into them, and then potentially pruning them
after the face. It is possible to build compressed quadtrees based upon sorting
points (see the section on z-order above) but pilot experiments found that
constructing quadtrees this way is faster, but resulted in lower quality caches
so that overall running time was slower.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/3d_qt_kt_pts10000_ctime.eps}
\caption{Build Time(3d, pts=10000)}
\label{fig:3d_kd_qt_ctime}
\end{center}
\end{figure}

For query time it is found that for conditions under which the odds-on tree
performs well (sigma \(\le 0.1\)), the quadtree based implementation will
outperform the kd-tree based implementation.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/3d_qt_kt_pts10000_qtime.eps}
\caption{Query Time(3d, pts=10000)}
\label{fig:3d_kd_qt_qtime}
\end{center}
\end{figure}

Comparing total run times, the quadtree based implementation outperforms the
kd-tree based implementation for low entropy conditions (sigma \(\le 0.05\))
~\ref{fig:3d_kd_qt_total}. This is not the case in higher dimensions.
The quadtree based implementation is substantially worse than the kd-tree
base implementation for the \(d = 8\) case ~\ref{fig:8d_kd_qt_total}.  Quadtrees
exhibit best performance in small dimensions. This is in part because
the number of child nodes of a quadtree node is \(2^d\) for a \(d\)-dimensional
quadtree.

The kd-tree based implementation does not perform well in higher dimensional
spaces either so the quadtree remains a viable implementation choice over the
dimensions for which the odds-on tree exhibits good performance.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/3d_qt_kt_pts10000_total.eps}
\caption{Total Time (3d, pts=10000)}
\label{fig:3d_kd_qt_total}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/8d_qt_kt_pts10000_total.eps}
\caption{Total Time (8d, pts=10000)}
\label{fig:8d_kd_qt_total}
\end{center}
\end{figure}

Cache hit rate measures the number of times the odds-on tree is able to answer
a search without making use of the backup kd-tree structure. In general, the
quadtree has a better hit rate than the kd-tree based odds-on tree. Typical
results are shown in ~\ref{fig:2d_kd_qt_hits}.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/2d_qt_kt_pts10000_hits.eps}
\caption{Cache Hits (2d, pts=10000)}
\label{fig:2d_kd_qt_hits}
\end{center}
\end{figure}

For a node to be marked terminal the area it covers must have the same nearest
neighbour. Since the interference queries are run from the corners of the node,
a certain area outside of the node must also have the same nearest neighbour.
A square node requires less area outside of the node to have the same nearest
neighbour and so is more likely to be marked terminal. The area that must have
the same nearest neighbour for a square and for a rectangular node is
illustrated in ~\ref{fig:node_coverage}.

A quadtree node always has equal length sides, but a kd-tree node is built
by sub-dividing points and may have some sides much longer than others. Because
of this, given a quadtree node and a kd-tree node of the same area the kd-tree
node is less likely to be marked terminal, which means that a kd-tree based
odds-on tree implementation is likely to have a greater number of smaller nodes,
which will cause a lower cache hit rate.

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{diagrams/node_coverage.eps}
\caption{Square vs Rectangular Nodes}
\label{fig:node_coverage}
\end{center}
\end{figure}

The quadtree based implementation is able to build a more efficient cache than
the kd-tree based implementation. Because of this, it exhibits better
performance than the kd-tree based implementations for small values of search
set sigma.

\chapter{Conclusions}

\section{Summary}

We have shown in this thesis that it is possible to create a practical data
structure for biased neighbour search based upon odds-on trees. We developed
implementations based upon quadtrees and kd-trees and through a series of
careful experiments, compared their performance to our kd-tree implementation,
on a number of synthetic datasets.

Our primary metric was running time but we also considered cache hit ratios,
when comparing performance between odds-on tree implementations.

We found both implementations would outperform the kd-tree provided the entropy
of the set of sites returned under the search distribution is sufficiently
small. For small dimensions and sufficiently low search entropy, we found that
the quadtree will outperform the kd-tree based implementation. For large
dimensions, neither implementation performed bettered than a kd-tree.

Increasing the number of samples used to build the odds-on tree increased
build time but resulted in building a better cache. Adjusting the maximum
build depth did not have a clear effect on the build times or the quality of the
cache. Increasing the point set size did not change the relative performance
of the implementations.

\section{Future Work}

The experimentation in this thesis focused on synthetic datasets due to the
difficulty in finding ``real world'' datasets that included search distributions
as well as the underlying point sets. Further experimentation should be done
if suitable non-synthetic datasets are found.

The quadtree based implementation outperformed the kd-tree based implementation
in some cases due to creating nodes that have equal side length. Our kd-tree
implementation followed the original strategy in \cite{kdtree} of sorting the
points into two equal sized groups. In applications of kd-trees, particularly
in graphics, a number of other heuristic approaches have been developed that 
attempt to avoid creating nodes with a great disparity in side length
\cite{physicallybasedrendering}. Using these heuristics might provide better
performance than either a quadtree or a standard kd-tree.

The Balanced Box Decomposition (BBD) Tree \cite{optimalann} is a more
complicated data structure that maintains the equal space and equal partition
size properties of quadtrees and kd-trees. It was not implemented as part of
this thesis as its reported performance in the literature did not seem to
justify dealing with the increased complexity of implementation. Given the
results found for quadtrees and kd-trees it now seems worthwhile investigating.

This thesis did not investigate the k nearest neighbour problem, where rather
than returning a single nearest neighbour instead k nearest neighbours are
returned. Intuitively, increasing k should have a negative impact on the cache
as the area covered by a node that has the same k nearest neighbours will be
smaller than the area covered by a node with only a single nearest neighbour.
This could be investigated experimentally.

A large portion of the time in doing a k-nearest neighbour search is spent in
backtracking, where other potential nearest neighbours are investigated after
the candidate nearest neighbours have been identified. The odds-on tree could
potentially be used to prune the number of nodes investigated during back
tracking, which might improve performance.

This thesis also did not investigate approximate nearest neighbour search. An
approximate nearest neighbour search often dramatically reduces the amount of
backtracking required to identify nearest neighbours and performs better in
higher dimensional spaces. The straightforward approach would be to mark nodes
as terminal that have the same approximate nearest neighbours.

Approximate Voronoi Diagrams \cite{avd} were not considered for implementation
in this thesis as they only work in the approximate case, but would be
worthwhile investigating in the future.

\begin{thebibliography}{5}

\bibitem{treap}
Aragon, C. R.,  Seidel, R. (1996) Randomized Search Trees.
In Algorithmica, Vol. 16, Number 4/5, pp. 464--497

\bibitem{fastvector}
Arya, S., Mount, D. M. (1993) Algorithms for Fast Vector Quantization,
Proceedings of DCC '93, IEEE Press, pp 381--390. 

\bibitem{app-ann}
Arya, S., Mount, D. M. (1993) Approximate nearest neighbour queries in Fixed
Dimensions, Proceedings of the 4th ACIM-SIAM Symposium on Discrete Algorithms,
pp 271 -- 280.

\bibitem{optimalann}
Arya, S., Mount, D. M., Netanyahu, N. S., Silverman, R. Wu, A. Y. (1998)
An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed
Dimensions, Journal of the ACM, Volume 45, Issue 6, pp 891--923

\bibitem{simpleentropy}
Arya, S., Malamatos, T. Mount, D. M. (2001) A Simple Entropy-Based Algorithm
for Planar Point Location. In Proceedings of the Twelfth Annual ACM-SIAM
Symposium on Discrete Algorithms, pp. 262--268

\bibitem{arya-avd}
Arya, S., Malamatos, T. Mount, D. M. (2002) Space-efficient Approximate
Voronoi Diagrams, Proceedings of the 33rd Annual ACM Symposium on the Theory
of Computing, pp. 721--730.

\bibitem{simpleentropy}
Arya, S., Malamatos, T., Mount, D. M. (2001) A simple entropy-based algorithm
for planar point location.
In Proceedings of the Twelfth Annual ACM-SIAM Symposium on Discrete Algorithms,
pp. 262-268

\bibitem{bsl}
Bagchi, A. Buchsbaum, A. L., Goodrich, M. T. (2005) Biased Skip Lists.
In Algorithmica Vol. 42, pp. 31  48

\bibitem{bst}
Bent, S. W., Sleator, D. D., Tarjan, R. E. (1984) Biased Search Trees,
In SIAM Journal on Computing, Volume 14, pp. 545 - 568

\bibitem{kdtree}
Bentley, J. L. (1975) Multidimensional Binary Search Trees Used for Associative
Searching, In Communications of the ACM, Volume 18, Number 9, pp. 509--517 

\bibitem{rangetree}
Bentley, J. L. (1980) Multidimensional Divide-and-Conquer.
In Communications of the ACM, Volume 23, pp. 214--229.

\bibitem{bern}
Bern, M., Eppstein, D., Teng, S-H. (1993) Parallel Construction of Quadtrees and
Quality Triangulations, WADS '93: Proceedings of the Third Workshop on
Algorithms and Data Structures, pp. 188 -- 199. 

\bibitem{oddson}
Bose, P., Devroye, L, Douieb, K., Dujmovic, V., King, J., Morin, P. (2010)
Odds-On Trees, preprint, retrieved from http://arxiv.org/abs/1002.1092 on
April 27, 2013.

\bibitem{chan}
Chan, T. M. (2006) A Minimalist's Implementation of an Approximate Nearest
Neighbour Algorithm in Fixed Dimensions, unpublished note, retrieved from
\url{http://www.cs.uwaterloo.ca/~tmchan/pub\_ann.html} on July 29, 2012

\bibitem{chazelle} Chazelle, B. (1993) Cutting hyperplanes for divide-and-conquer.
Discrete and Computational Geometry 9, pp. 145--158.

\bibitem{compressedquadtree}
Clarkson, Kenneth L. (1983) Fast Algorithms for the All Nearest Neighbors Problem,
FOCS '83: Proceedings of the Twenty-fourth Symposium on Foundations of
Computer Science, Tucson, AZ 

\bibitem{connor}
Connor, M. and Kumar, P. (2009) Fast Construction of k-Nearest Neighbor Graphs
For Point Clouds, IEEE Transactions of Visualization and Computer Graphics,
September 2009.

\bibitem{dutch}
de Berg., M. van Kreveld, M., Overmars, M., Schwarzkopf, O. (2000)
Computational Geometry: Algorithms and Applications, 2nd Edition, Springer-Verlag,
Berlin Heidelberg New York

\bibitem{biasedrange}
Dujmovic, V., Howat, J., Morin, P. (2009) Biased Range Trees
In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,
New York, New York, pp. 486 -- 495

\bibitem{edelsbrunner}
Edelsbrunner, H., Guibas, L. J., Stolfi, J. (1986) Optimal Point Location in a
Monotone Subdivision, SIAM Journal on Computing, 15(2), pp. 317--340. 

\bibitem{skipquadtree}
Eppstein, D., Goodrich, M. T., Sun, J. Z. (2008) The Skip Quadtree: A Simple
Dynamic Data Structure for Multidimensional Data, Int. Journal on Computational
Geometry and Applications, 18(1/2), pp. 131--160 

\bibitem{bsl2}
Ergun, F., Sahinalp, S. C., Sharp, J., Sinha, R. K. (2001) Biased Skip Lists
for Highly Skewed Access Patterns, Proceedings of ALENEX 2001, Lecture Notes in
Computer Science, Volume 2153, pp. 216--229 

\bibitem{bst2}
Feigenbaum, J., Tarjan, R. E. (1983) Two New Kinds of Biased Search Tree,
In Bell Systems Technical Journal, Vol. 62, No. 10, pp. 3139 -- 3158

\bibitem{quadtree}
Finkel, R. A. and Bentley, J. L. (1974) Quad Trees A Data Structure for
Retrieval on Composite Keys, Acta Informatica, 4(1), pp. 1 - 9 

\bibitem{friedman}
Friedman, J. H., Bentley, J. L., Finkel, R. A. (1977) An Algorithm for Finding
Best Matches in Logarithmic Expected Time, ACM Transactions on Mathematical
Software, Vol. 3 Number 3, pp. 209 -- 226

\bibitem{rtree}
Guttman, A. (1984) R-Trees: A Dynamic Index Structure for Spatial Searching,
SIGMOD '84: Proceedings of the 1984 ACM SIGMOD International Conference on the
Management of Data, pp. 47 -- 58

\bibitem{avd}
Har-Peled, S. (2001) A Replacement for Voronoi Diagrams of Near Linear Size, 
FOCS '01: Proceedings of the Symposium on Foundations of Computer Science, 
pp. 94 -- 103

\bibitem{iacono}
Iacono, J. (2001) Optimal Planar Point Location
In Proceedings of the Twelfth Annual ACM-SIAM Symposium on Discrete Algorithms,
pp. 240-241

\bibitem{lsh}
Indyk, P., Motwani, R. (1998) Approximate Nearest Neighbors: Towards Removing
the Curse of Dimensionality, In Proceedings of the Thirtieth Annual ACM
Symposium on Theory of Computing, pp. 604 -- 613 

\bibitem{kirkpatrick}
Kirkpatrick, D. (1983) Optimal Search in Planar Subdivisions,
Siam Journal of Computing, Vol. 12, No. 1, pp. 28 -- 34.

\bibitem{knuth}
Knuth, D. E. (1998) The Art of Computer Programming, Second Edition,
Volume 3, Sorting and Searching.  Addison-Wesley, Reading, Massachusetts, USA.

\bibitem{practicalann}
Liu, T., Moore, A. W., Gray, A., Yang, K. (2004) An Investigation of Practical
Approximate Nearest-Neighbour Algorithms, Proceedings of Neural Information
Processing Systems(NIPS 2004), pp. 825 -- 832 

\bibitem{mehlhorn}
Mehlhorn, K. (1975) Nearly Optimal Binary Search Trees 
In Acta Informatica, 5(1), pp. 287  295

\bibitem{morton}
Morton, G. M. (1966), A Computer Oriented Geodetic Data Base; and a New
Technique in File Sequencing, Technical Report, IBM Ltd., Ottawa, Canada

\bibitem{ann}
Mount, D. M., Arya, S. (2012) ANN: A Library for Approximate Nearest Neighbor
Searching, retrieved from \url{http://www.cs.umd.edu/~mount/ANN} on July 29, 2012 

\bibitem{physicallybasedrendering}
Pharr, M., Humphreys, G. (2010) Physically Based Rendering
Morgan-Kaufmann, San Francisco, CA.  

\bibitem{skiplist}
Pugh, W. (1990) Skip Lists: A Probabilistic Alternative to Balanced Trees.
In Communications of the ACM, 33(6), pp. 668  676

\bibitem{samet}
Samet, H. (2006) Foundations of Multidimensional and Metric Data Structures
Morgan-Kaufmann, San Francisco, CA.

\bibitem{sarnak}
Sarnak, N. and Tarjan, R. E. (1986) Planar Point Location Using Persistent
Search Trees. Communications of the ACM, 29(7), pp. 669 -- 679

\bibitem{claudeshannonwasagod}
Shannon, Claude E. (1948) A Mathematical Theory of Communication,
Bell System Technical Journal, 27, pp. 379--423 and 623--656.

\bibitem{splaytree}
Sleator, D. D., Tarjan, R. E. (1985) Self-adjusting binary search trees.
In Journal of the ACM, 32, pp. 652  686

\bibitem{satp}
Williams, H. E., Zobel, J., Heinz, S. (2001) Self-Adjusting Trees in Practice
for Large Text Collections, In Software - Practice and Experience,
Volume 31, pp. 925 - 939

\end{thebibliography}

\end{document}
